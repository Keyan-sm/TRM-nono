	1.	Understand the training dataset thoroughly in @NonoDataset-main. This is confusing, it is split into many files. . 

	2.	Test Gemini 3 performance on the training dataset 
        - look at gem3 on 10x10 then 15x15 boards
        - This may be incomplete/anicdotal test just to see where we are at
        - We can poss hold off on full test acuracy of gem3 and opus 4.5 until later
        - just need to know the baseline so we can understand how to think abt training TRM on this set 
        - IE if gem3 is 90% acc on 10x10 boards, then we know we need to beat that or go to 15x15 board 
	3.	Based on Geminiâ€™s performance, determine how the TRM architecture for this task should differ (if at all) from:
	    - The published architectures in the Samsung paper

1. prev attempt at training TRM on this dataset failed.  
a- ([Rank 0, World Size 1]: Epoch 49
TRAIN
EVALUATE
SWITCH TO EMA
Skipping evaluation (no eval loader)
SAVE CHECKPOINT
0it [05:11, ?it/s]
keyanmikaili@Keyans-Mac-mini TinyRecursiveModels-main(Samsung Paper Repo) %)
b- 
Inference Results (Epoch 1):

Prediction: Still predicting all zeros.
Accuracy: 33.00%.
It seems the model hasn't broken out of the "safe" zero-prediction strategy yet. This can happen in the early stages, especially if the class imbalance (more zeros than ones, or vice versa) pushes it towards a default value, or if the reasoning depth needs more time to stabilize.



2. Use the @10x10 dataset to train a new simple proof of concept TRM model. 
- create in a new folder under TRMProj, create copies of relevant scripts. Do not interfere with or edit the current codebase. 
    - proof of concept means we don't need to get to 100% accuracy, we just need to show that it can learn something; train on a small portion of the entire dataset (10%?)
    - Use the same architecture and methodology as the published Samsung paper
    - Note the format of the @10x10 dataset 